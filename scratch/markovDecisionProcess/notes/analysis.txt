


Bridge Crossing Analysis:
The agent will attempt to cross the bridge if there is low noise and it highly likely to move in its intended direction. 

Therefore, it will cross the bridge if the intended movement probability is 0.99

____________________________________________________________________
Input:
alpha =  0.9 
moveProb = 0.99


Output: 
After  100  iterations: 
['OBS', '-100', '-100', '-100', '-100', '-100', 'OBS']
['+1', 0.41416665740761455, 1.58893460207958, 3.0539152039899995, 4.8808020999999995, 7.159, '+10']
['OBS', '-100', '-100', '-100', '-100', '-100', 'OBS']


['OBS', '-100', '-100', '-100', '-100', '-100', 'OBS']
['+1', 'right', 'right', 'right', 'right', 'right', '+10']
['OBS', '-100', '-100', '-100', '-100', '-100', None]
_____________________________________________________________________

Policies Experiments

a) Prefer the close exit (+1), risking the cliff (-10)

	discount/alpha = 0.9
	*living reward = -3.05
	intended move probability = 0.8


	After  100  iterations: 
	[-10.06, -8.13, -5.81, -4.22, -1.13]
	[-11.42, 'OBS', -2.71, -1.13, 3.63]
	[-10.46, 'OBS', '+1', 'OBS', '+10']
	[-8.82, -6.35, -3.05, -1.68, 3.58]
	['-10', '-10', '-10', '-10', '-10']


	['right', 'right', 'down', 'right', 'down']
	['up', 'OBS', 'down', 'right', 'down']
	['down', 'OBS', '+1', 'OBS', '+10']
	['right', 'right', 'up', 'right', 'up']
	['-10', '-10', '-10', '-10', '-10']


b) Prefer the close exit (+1), but avoiding the cliff (-10)
	*discount/alpha = 0.4
	living reward = -0.05
	intended move probability = 0.8



	fter  100  iterations: 
	[-0.06, -0.06, -0.04, -0.03, 0.11]
	[-0.06, 'OBS', 0.08, 0.11, 1.25]
	[-0.06, 'OBS', '+1', 'OBS', '+10']
	[-0.06, -0.06, 0.08, -0.03, 1.25]
	['-10', '-10', '-10', '-10', '-10']


	['up', 'right', 'down', 'right', 'down']
	['up', 'OBS', 'down', 'right', 'down']
	['up', 'OBS', '+1', 'OBS', '+10']
	['up', 'up', 'up', 'up', 'up']
	['-10', '-10', '-10', '-10', '-10']


c) Prefer the distant exit (+10), risking the cliff (-10)
	discount/alpha = 0.9
	living reward = -0.05
	*intended move probability = 0.99

	After  100  iterations: 
	[2.61, 3.29, 4.13, 5.16, 6.44]
	[2.06, 'OBS', 5.14, 6.44, 8.03]
	[2.45, 'OBS', '+1', 'OBS', '+10']
	[3.09, 3.95, 5.02, 6.37, 8.03]
	['-10', '-10', '-10', '-10', '-10']


	['right', 'right', 'right', 'right', 'down']
	['up', 'OBS', 'right', 'right', 'down']
	['down', 'OBS', '+1', 'OBS', '+10']
	['right', 'right', 'right', 'right', 'up']
	['-10', '-10', '-10', '-10', '-10']




d) Prefer the distant exit (+10), avoiding the cliff (-10)
	discount/alpha = 0.9
	living reward = -0.05
	intended move probability = 0.8


	After  100  iterations: 
	[1.87, 2.56, 3.39, 4.4, 5.61]
	[1.39, 'OBS', 3.94, 5.61, 7.49]
	[1.01, 'OBS', '+1', 'OBS', '+10']
	[0.7, 0.47, 1.98, 4.26, 7.37]
	['-10', '-10', '-10', '-10', '-10']


	['right', 'right', 'right', 'right', 'down']
	['up', 'OBS', 'right', 'right', 'down']
	['up', 'OBS', '+1', 'OBS', '+10']
	['up', 'up', 'right', 'right', 'up']
	['-10', '-10', '-10', '-10', '-10']




e) Avoid both exits and the cliff (so an episode should never terminate)
	discount/alpha = 0.9
	*living reward = 10.05
	intended move probability = 0.8


	After  100  iterations: 
	[52.87, 52.87, 52.87, 52.87, 52.87]
	[52.87, 'OBS', 52.87, 52.87, 52.87]
	[52.87, 'OBS', '+1', 'OBS', '+10']
	[52.65, 50.3, 41.92, 47.6, 40.89]
	['-10', '-10', '-10', '-10', '-10']

	['up', 'up', 'up', 'up', 'up']
	['up', 'OBS', 'up', 'up', 'up']
	['up', 'OBS', '+1', 'OBS', '+10']
	['up', 'up', 'left', 'up', 'left']
	['-10', '-10', '-10', '-10', '-10']







